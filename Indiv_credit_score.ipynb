{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##  Importing necessary libraries"
      ],
      "metadata": {
        "id": "OoxSRxVw0PrX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wwXzU5Po3Gx"
      },
      "outputs": [],
      "source": [
        "# Importing all the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import metrics\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import normalize\n",
        "import scipy.stats\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data Understanding"
      ],
      "metadata": {
        "id": "oianbP250KQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"loan-train.csv\")\n",
        "test= pd.read_csv(\"loan-test.csv\")"
      ],
      "metadata": {
        "id": "JNmG-JliqMyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n"
      ],
      "metadata": {
        "id": "yTt7AHJWqSs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n"
      ],
      "metadata": {
        "id": "mdD40Js-sszA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# @title Gender\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "df.groupby('Gender').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "nmR6gsbbrw2m"
      }
    },
    {
      "source": [
        "# @title LoanAmount vs Loan_Amount_Term\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "df.plot(kind='scatter', x='LoanAmount', y='Loan_Amount_Term', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "FvAplIPMrfIF"
      }
    },
    {
      "source": [
        "# @title Loan_Amount_Term\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "df['Loan_Amount_Term'].plot(kind='hist', bins=20, title='Loan_Amount_Term')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "K0kJqQWBrQ6v"
      }
    },
    {
      "source": [
        "# @title LoanAmount\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "df['LoanAmount'].plot(kind='hist', bins=20, title='LoanAmount')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "NzT1VlS7q92x"
      }
    },
    {
      "source": [
        "# @title CoapplicantIncome\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "df['CoapplicantIncome'].plot(kind='hist', bins=20, title='CoapplicantIncome')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "iysFSfVmq32n"
      }
    },
    {
      "source": [
        "# @title ApplicantIncome\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "df['ApplicantIncome'].plot(kind='hist', bins=20, title='ApplicantIncome')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "09Sr2KHtqxH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "1at37SsuqUrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization 1: Loan Approval Status Distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='Loan_Status', data=df)\n",
        "plt.title('Loan Approval Status Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "21TkdqiOqgX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['ApplicantIncome'], bins=30, kde=True)\n",
        "plt.title('Applicant Income Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2oandpRRsZvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization 3: Loan Amount Distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['LoanAmount'].dropna(), bins=30, kde=True)\n",
        "plt.title('Loan Amount Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8BqCgOCrsfTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization 4: Credit History Distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='Credit_History', data=df)\n",
        "plt.title('Credit History Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z0G2yoUlsn5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization 5: Loan Approval Status based on Gender\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='Gender', hue='Loan_Status', data=df)\n",
        "plt.title('Loan Approval Status based on Gender')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r6gtuJHQstGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization 6: Loan Approval Status based on Marital Status\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='Married', hue='Loan_Status', data=df)\n",
        "plt.title('Loan Approval Status based on Marital Status')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DT-G8DHNsyHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization 7: Loan Approval Status based on Education\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='Education', hue='Loan_Status', data=df)\n",
        "plt.title('Loan Approval Status based on Education')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9GE3ije1s8LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization 8: Applicant Income vs. Loan Amount\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='ApplicantIncome', y='LoanAmount', data=df)\n",
        "plt.title('Applicant Income vs. Loan Amount')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KbH7iyADtDjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved Applicant Income vs. Loan Amount\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Scatter plot with points colored by Loan Approval Status\n",
        "scatter = sns.scatterplot(x='ApplicantIncome', y='LoanAmount', hue='Loan_Status', palette='Set1', data=df, alpha=0.7)\n",
        "\n",
        "# Highlighting the points with a legend\n",
        "scatter.legend(title='Loan Status', loc='upper right', labels=['Approved', 'Not Approved'])\n",
        "\n",
        "plt.title('Applicant Income vs. Loan Amount')\n",
        "plt.xlabel('Applicant Income')\n",
        "plt.ylabel('Loan Amount')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ia-NISBjtZXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization 9: Loan Approval Status based on Credit History\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='Credit_History', hue='Loan_Status', data=df)\n",
        "plt.title('Loan Approval Status based on Credit History')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GVxvLVrjtISC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization: Applicant Income vs. Loan Approval Status\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.violinplot(x='Loan_Status', y='ApplicantIncome', data=df, palette='Set1')\n",
        "plt.title('Applicant Income vs. Loan Approval Status')\n",
        "plt.xlabel('Loan Approval Status')\n",
        "plt.ylabel('Applicant Income')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gd0Ea48xuhBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization: Education vs. Loan Approval Status\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='Education', hue='Loan_Status', data=df, palette='Set2')\n",
        "plt.title('Education vs. Loan Approval Status')\n",
        "plt.xlabel('Education')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rggdqDWpuuJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualization: Loan Approval Status based on Property Area\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='Property_Area', hue='Loan_Status', data=df, palette='Set3')\n",
        "plt.title('Loan Approval Status based on Property Area')\n",
        "plt.xlabel('Property Area')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0yuYsAcvvq0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: display a graph showing the percentage of Loan Approval Status = Y And =N for each Property_Area\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a crosstab of Loan_Status and Property_Area\n",
        "crosstab = pd.crosstab(df['Loan_Status'], df['Property_Area'])\n",
        "\n",
        "# Calculate the percentage of each Loan_Status for each Property_Area\n",
        "crosstab_pct = crosstab.div(crosstab.sum(axis=1), axis=0)\n",
        "\n",
        "# Plot the results as a bar chart\n",
        "crosstab_pct.plot(kind='bar', stacked=True)\n",
        "plt.title('Percentage of Loan Approval Status for each Property Area')\n",
        "plt.xlabel('Property Area')\n",
        "plt.ylabel('Percentage')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_ayywXC2u61L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a crosstab of Loan_Status and Property_Area\n",
        "crosstab = pd.crosstab(df['Property_Area'], df['Loan_Status'])\n",
        "\n",
        "# Calculate the percentages for each Loan_Status and Property_Area\n",
        "crosstab_pct = crosstab.div(crosstab.sum(axis=1), axis=0)\n",
        "\n",
        "# Create a bar chart\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Set the x-axis labels\n",
        "ax.set_xticklabels(crosstab_pct.index, rotation=45, ha='right')\n",
        "\n",
        "# Plot the bars with light blue and dark blue colors\n",
        "ax.bar(crosstab_pct.index, crosstab_pct['Y'], label='Approved', color='lightblue')\n",
        "ax.bar(crosstab_pct.index, crosstab_pct['N'], bottom=crosstab_pct['Y'], label='Not Approved', color='darkblue')\n",
        "\n",
        "# Add a legend and title\n",
        "ax.legend()\n",
        "ax.set_title('Percentage of Loan Approval Status for each Property Area')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "D49a2DdLy_9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "checking Missing values"
      ],
      "metadata": {
        "id": "DdC-WfPD0e3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()/len(df)"
      ],
      "metadata": {
        "id": "MwqYntJCto_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned = df.dropna()"
      ],
      "metadata": {
        "id": "eN8-dzSR3oEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.isnull().sum()"
      ],
      "metadata": {
        "id": "b6Hjt4QcvN_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Transformation"
      ],
      "metadata": {
        "id": "2WHUncVXzbpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.info()"
      ],
      "metadata": {
        "id": "wcSFPKHI0DZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned"
      ],
      "metadata": {
        "id": "vm7tsZnx2ty5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#target encoding\n",
        "label_encoder = LabelEncoder()\n",
        "df_cleaned['Loan_Status'] = label_encoder.fit_transform(df_cleaned['Loan_Status'])\n"
      ],
      "metadata": {
        "id": "_cfHtbyI0vsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "df_cleaned.groupby('Loan_Status').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "metadata": {
        "id": "f_zmZ_K2oXv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned"
      ],
      "metadata": {
        "id": "KBJDBgV817In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_cleaned['Self_Employed'] = label_encoder.fit_transform(df_cleaned['Self_Employed'])\n",
        "df_cleaned['Education'] = label_encoder.fit_transform(df_cleaned['Education'])\n",
        "df_cleaned['Married'] = label_encoder.fit_transform(df_cleaned['Married'])"
      ],
      "metadata": {
        "id": "lhPlw4tJ2z_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned"
      ],
      "metadata": {
        "id": "tm9FP8v-4dEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.info()"
      ],
      "metadata": {
        "id": "SIAge6654fjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: show unique values of Dependents\n",
        "\n",
        "print(df_cleaned['Dependents'].unique())\n"
      ],
      "metadata": {
        "id": "UWWzPYPYb7Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_cleaned is your DataFrame containing the 'Dependents' column\n",
        "\n",
        "# Define a mapping dictionary\n",
        "dependents_map = {'0': 0, '1': 1, '2': 2, '3+': 3}\n",
        "\n",
        "# Map the values using the dictionary and replace the column in df_cleaned\n",
        "df_cleaned['Dependents'] = df_cleaned['Dependents'].map(dependents_map)\n"
      ],
      "metadata": {
        "id": "8sL3EAqIcjul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned"
      ],
      "metadata": {
        "id": "i5bFnNLCdkyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "drop loan_id column"
      ],
      "metadata": {
        "id": "JVTxRHY240bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned = df_cleaned.drop('Loan_ID', axis=1)\n"
      ],
      "metadata": {
        "id": "kb09giPl4irK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.info()"
      ],
      "metadata": {
        "id": "a5ah4Idk5QSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: select categorical columns from df_cleaned and apply one hot encoding on them\n",
        "\n",
        "categorical_columns = df_cleaned.select_dtypes(include=['object']).columns\n",
        "df_cleaned = pd.get_dummies(df_cleaned, columns=categorical_columns)\n"
      ],
      "metadata": {
        "id": "9mMwAYkL5V3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.head()"
      ],
      "metadata": {
        "id": "ddKoIjMN5xuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.shape"
      ],
      "metadata": {
        "id": "OIqk6rHIJUWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform one-hot encoding for Property_Area\n",
        "df_cleaned = pd.get_dummies(df_cleaned, columns=['Property_Area_Rural', 'Property_Area_Semiurban', 'Property_Area_Urban'], drop_first=True)\n",
        "\n",
        "# Display the DataFrame after encoding\n",
        "print(df_cleaned)"
      ],
      "metadata": {
        "id": "1eddjnJTe6oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned"
      ],
      "metadata": {
        "id": "ET1VUyyCZawn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Selection"
      ],
      "metadata": {
        "id": "0seJzp8ZnHCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert boolean columns to numerical\n",
        "df_cleaned['Gender_Female'] = df_cleaned['Gender_Female'].astype(int)\n",
        "df_cleaned['Gender_Male'] = df_cleaned['Gender_Male'].astype(int)\n",
        "df_cleaned['Property_Area_Rural_True'] = df_cleaned['Property_Area_Rural_True'].astype(int)\n",
        "df_cleaned['Property_Area_Semiurban_True'] = df_cleaned['Property_Area_Semiurban_True'].astype(int)\n",
        "df_cleaned['Property_Area_Urban_True'] = df_cleaned['Property_Area_Urban_True'].astype(int)\n",
        "\n",
        "# Display the DataFrame after transformation\n",
        "print(df_cleaned)\n"
      ],
      "metadata": {
        "id": "srJ65StygHtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned"
      ],
      "metadata": {
        "id": "R5toE3TxfEH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iv_woe(data, target, bins=10, show_woe=False):\n",
        "    # Empty Dataframe\n",
        "    newDF, woeDF = pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    # Extract Column Names\n",
        "    cols = data.columns\n",
        "\n",
        "    # Run WOE and IV on all the independent variables\n",
        "    for ivars in cols[~cols.isin([target])]:\n",
        "        if (data[ivars].dtype.kind in 'bifc') and (len(np.unique(data[ivars])) > 10):\n",
        "            binned_x = pd.qcut(data[ivars], bins, duplicates='drop')\n",
        "            d0 = pd.DataFrame({'x': binned_x, 'y': data[target]})\n",
        "        else:\n",
        "            d0 = pd.DataFrame({'x': data[ivars], 'y': data[target]})\n",
        "        d0 = d0.astype({\"x\": str})\n",
        "        d = d0.groupby(\"x\", as_index=False, dropna=False).agg({\"y\": [\"count\", \"sum\"]})\n",
        "        d.columns = ['Cutoff', 'N', 'Events']\n",
        "        d['% of Events'] = np.maximum(d['Events'], 0.5) / d['Events'].sum()\n",
        "        d['Non-Events'] = d['N'] - d['Events']\n",
        "        d['% of Non-Events'] = np.maximum(d['Non-Events'], 0.5) / d['Non-Events'].sum()\n",
        "        d['WoE'] = np.log(d['% of Non-Events'] / d['% of Events'])\n",
        "        d['IV'] = d['WoE'] * (d['% of Non-Events'] - d['% of Events'])\n",
        "        d.insert(loc=0, column='Variable', value=ivars)\n",
        "\n",
        "        # Calculate total IV for each group in each column\n",
        "        d['Total IV'] = d.groupby('Variable')['IV'].transform('sum')\n",
        "\n",
        "        total_iv = d['IV'].sum()\n",
        "\n",
        "        # Check if IV is above threshold\n",
        "\n",
        "        print(\"Information value of \" + ivars + \" is \" + str(round(total_iv, 6)))\n",
        "        temp = pd.DataFrame({\"Variable\": [ivars], \"IV\": [total_iv]}, columns=[\"Variable\", \"IV\"])\n",
        "        newDF = pd.concat([newDF, temp], axis=0)\n",
        "        woeDF = pd.concat([woeDF, d], axis=0)\n",
        "\n",
        "            # Show WOE Table\n",
        "        if show_woe:\n",
        "          print(d)\n",
        "\n",
        "    return newDF, woeDF\n",
        "\n",
        "\n",
        "# Call the function and drop weak weight columns with IV less than 0.05\n",
        "iv, woe = iv_woe(data=df_cleaned, target='Loan_Status', bins=10, show_woe=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yJ_RE_4ep2mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_total_iv_values = woe['Total IV'].unique()\n",
        "\n",
        "# Print the unique values\n",
        "print(\"Unique values of Total IV column:\")\n",
        "print(unique_total_iv_values)\n",
        "\n",
        "print(unique_total_iv_values)\n",
        "col=df_cleaned.columns.tolist()"
      ],
      "metadata": {
        "id": "6xWVwKSLq3S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AhQ7aXHfnmOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.head()"
      ],
      "metadata": {
        "id": "AJdriOXNl5WR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping features\n",
        "#By human measures , we are going to keep the Married,Applicant Income feature\n",
        "# By the threshold measure Self employed,Gender Male,Female,Dependents,CoapplicantIncome,Education,Property_Area_Rural_True,Property_Area_Urban_True are going to be dropped\n",
        "#by threshold , we are going to keep Loan Amount,Loan_Amount_Term,Credit History,Property_Area_Semiurban_True\n",
        "columns=['Gender_Female','Gender_Male','Self_Employed','Dependents','CoapplicantIncome','Education','Property_Area_Rural_True','Property_Area_Urban_True']\n",
        "\n",
        "df_cleaned_drop1=df_cleaned.drop(columns,axis=1)\n",
        "df_cleaned_drop1['Property_Area']=df_cleaned_drop1['Property_Area_Semiurban_True']\n",
        "df_cleaned_drop1 = df_cleaned_drop1.drop('Property_Area_Semiurban_True', axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dws1NZrTiAmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned_drop1.head()"
      ],
      "metadata": {
        "id": "TkWonk-NmwkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned_dropped=df_cleaned_drop1\n"
      ],
      "metadata": {
        "id": "DGL7n2d0uqb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scaling"
      ],
      "metadata": {
        "id": "8BX2vo_q4cSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: scale the dataset df_cleaned_dropped using minmaxscaler\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_cleaned_scaled = scaler.fit_transform(df_cleaned_dropped)\n",
        "df_cleaned_scaled = pd.DataFrame(df_cleaned_scaled, columns=df_cleaned_dropped.columns)\n",
        "df_cleaned_scaled\n"
      ],
      "metadata": {
        "id": "45hamivK6MYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Score Prediction"
      ],
      "metadata": {
        "id": "rztPdUom9K_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LogisticRegression"
      ],
      "metadata": {
        "id": "6mx9z-MtA1sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df_cleaned_scaled.drop(columns=['Credit_History'])  # Features\n",
        "y = df_cleaned_scaled['Credit_History']  # Target\n",
        "\n",
        "# Initialize logistic regression model\n",
        "log_reg = LogisticRegression()\n",
        "\n",
        "# Fit the model\n",
        "log_reg.fit(X, y)\n",
        "\n",
        "# Predict probabilities\n",
        "probabilities = log_reg.predict_proba(X)[:, 0]  # Probability of class 1 (Loan_Status = 1)\n",
        "\n",
        "# Add probabilities as a new column to the DataFrame\n",
        "df_cleaned_scaled['Probability'] = probabilities\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(\"Updated DataFrame with Probability column:\")\n",
        "df_cleaned_scaled\n",
        "\n"
      ],
      "metadata": {
        "id": "iirDAgDj6Zma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Model Evaluation\n",
        "predictions = log_reg.predict(X)\n",
        "accuracy = accuracy_score(y, predictions)\n",
        "precision = precision_score(y, predictions)\n",
        "recall = recall_score(y, predictions)\n",
        "f1 = f1_score(y, predictions)\n",
        "roc_auc = roc_auc_score(y, probabilities)\n",
        "\n",
        "print(\"Model Evaluation Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "metadata": {
        "id": "kMWDGnddMyen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = df_cleaned_scaled['Loan_Status']"
      ],
      "metadata": {
        "id": "_dvXp5cfD-T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw,X_test,y_raw,y_test  = train_test_split(df_cleaned_scaled,\n",
        "                                              target,\n",
        "                                              test_size=0.1,\n",
        "                                              stratify = target,\n",
        "                                              random_state = 42)"
      ],
      "metadata": {
        "id": "ixYDmOr1A9YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Initialize StratifiedKFold with shuffle=True\n",
        "sss = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "# Initialize an empty list to store accuracy scores for each fold\n",
        "accuracy_scores = []\n",
        "\n",
        "# Iterate over each fold\n",
        "for fold, (train_index, test_index) in enumerate(sss.split(X_raw, y_raw), 1):\n",
        "    print(f\"Fold {fold}:\")\n",
        "    X_train, X_val = X_raw.iloc[train_index], X_raw.iloc[test_index]\n",
        "    y_train, y_val = y_raw.iloc[train_index], y_raw.iloc[test_index]\n",
        "\n",
        "    # Initialize and fit logistic regression model\n",
        "    log_reg_cv = LogisticRegression(random_state=0)\n",
        "    log_reg_cv.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on validation set\n",
        "    y_pred = log_reg_cv.predict(X_val)\n",
        "\n",
        "    # Calculate accuracy score for this fold\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# Print mean accuracy across all folds\n",
        "mean_accuracy = np.mean(accuracy_scores)\n",
        "print(f\"Mean Accuracy: {mean_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "6QHT-XCpEOGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2IMtQMO6EzUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RandomForestReg"
      ],
      "metadata": {
        "id": "E5p_UUOf-WVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Step 1: Split the data into features (X) and target variable (y)\n",
        "X = df_cleaned_scaled.drop(columns=['Probability'])  # Features\n",
        "y = df_cleaned_scaled['Probability']  # Target variable\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train a RandomForestRegressor model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Extract feature importance\n",
        "feature_importance = model.feature_importances_\n",
        "\n",
        "# Create a DataFrame to store feature importance scores\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': feature_importance\n",
        "})\n",
        "\n",
        "# Sort features by importance score in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Step 5: Visualize feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AL4lpfWeA49a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Split the data into features (X) and target variable (y)\n",
        "X = df_cleaned_scaled.drop(columns=['Probability'])  # Features\n",
        "y = df_cleaned_scaled['Probability']  # Target variable\n",
        "\n",
        "# Step 2: Split the data into training and testing sets (optional, if you still want to have a separate test set)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train a RandomForestRegressor model with cross-validation\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)  # 5-fold cross-validation\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean CV score:\", np.mean(cv_scores))\n",
        "\n",
        "# Step 4: Extract feature importance using the entire dataset (not just the training set)\n",
        "model.fit(X, y)\n",
        "feature_importance = model.feature_importances_\n",
        "\n",
        "# Create a DataFrame to store feature importance scores\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': feature_importance\n",
        "})\n",
        "\n",
        "# Sort features by importance score in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Step 5: Visualize feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ORhNAmHo-N0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Step 6: Evaluate the model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"R-squared (R2):\", r2)\n",
        "\n"
      ],
      "metadata": {
        "id": "EbzUmq1a_Uhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GradientBoost"
      ],
      "metadata": {
        "id": "ez3s5n-v-nMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming your data is stored in a DataFrame called df\n",
        "\n",
        "# Step 1: Split the data into features (X) and target variable (y)\n",
        "X = df_cleaned_scaled.drop(columns=['Probability'])  # Features\n",
        "y = df_cleaned_scaled['Probability']  # Target variable\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train a GradientBoostingRegressor model\n",
        "model = GradientBoostingRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Extract feature importance\n",
        "feature_importance = model.feature_importances_\n",
        "\n",
        "# Create a DataFrame to store feature importance scores\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': feature_importance\n",
        "})\n",
        "\n",
        "# Sort features by importance score in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Step 5: Visualize feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Vk7S7YPRDn-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming your data is stored in a DataFrame called df\n",
        "\n",
        "# Step 1: Split the data into features (X) and target variable (y)\n",
        "X = df_cleaned_scaled.drop(columns=['Probability'])  # Features\n",
        "y = df_cleaned_scaled['Probability']  # Target variable\n",
        "\n",
        "# Step 2: Split the data into training and testing sets (optional, if you still want to have a separate test set)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train a GradientBoostingRegressor model with cross-validation\n",
        "model = GradientBoostingRegressor(random_state=42)\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)  # 5-fold cross-validation\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean CV score:\", np.mean(cv_scores))\n",
        "\n",
        "# Step 4: Extract feature importance using the entire dataset (not just the training set)\n",
        "model.fit(X, y)\n",
        "feature_importance = model.feature_importances_\n",
        "\n",
        "# Create a DataFrame to store feature importance scores\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': feature_importance\n",
        "})\n",
        "\n",
        "# Sort features by importance score in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Step 5: Visualize feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9n9K1Px5-sOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Step 6: Evaluate the model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"R-squared (R2):\", r2)\n"
      ],
      "metadata": {
        "id": "Vp6We_qI_Dmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Given feature weights\n",
        "feature_weights = {\n",
        "    'Married': 0.057974,\n",
        "    'LoanAmount': 0.105560,\n",
        "    'Loan_Amount_Term': 0.129796,\n",
        "    'Credit_History': 1.431605,\n",
        "    'Property_Area': 0.118846,\n",
        "    'ApplicantIncome':  0.031081\n",
        "}\n",
        "\n",
        "# Given features (assuming it's a DataFrame with columns matching feature names)\n",
        "features_df = pd.DataFrame({\n",
        "    'Married': [1.0],\n",
        "    'LoanAmount': [0.201354],\n",
        "    'Loan_Amount_Term': [0.72973],\n",
        "    'Credit_History': [1.0],\n",
        "    'Property_Area': [0.0],\n",
        "    'ApplicantIncome': [0.05483]\n",
        "})\n",
        "# Function to calculate credit score for a single row\n",
        "def calculate_credit_score(row):\n",
        "    return sum(row[feature] * weight for feature, weight in feature_weights.items())\n",
        "\n",
        "# Apply the function to each row to calculate credit scores\n",
        "df_cleaned_scaled['score'] = df_cleaned_scaled.apply(calculate_credit_score, axis=1)\n",
        "\n",
        "print(df_cleaned_scaled)\n"
      ],
      "metadata": {
        "id": "weH3MDQDD0Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned_scaled.describe()"
      ],
      "metadata": {
        "id": "IebaIwTzL4AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the desired range for the score\n",
        "min_score = 300\n",
        "max_score = 900\n",
        "\n",
        "# Calculate the min and max values of the current 'score' column\n",
        "min_current = df_cleaned_scaled['score'].min()\n",
        "max_current = df_cleaned_scaled['score'].max()\n",
        "\n",
        "# Perform min-max scaling\n",
        "df_cleaned_scaled['score_scaled'] = min_score + ((df_cleaned_scaled['score'] - min_current) * (max_score - min_score)) / (max_current - min_current)\n",
        "\n",
        "# Display the updated DataFrame with the scaled score\n",
        "print(\"Updated DataFrame with scaled score:\")\n",
        "df_cleaned_scaled\n"
      ],
      "metadata": {
        "id": "PEMlbUkUN3b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Score using Credit utlization ratio **"
      ],
      "metadata": {
        "id": "lxWmKSDGXFe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned_scaled1 = df_cleaned_scaled.copy()\n",
        "del df_cleaned_scaled1['score']\n",
        "del df_cleaned_scaled1['score_scaled']\n",
        "df_cleaned_scaled2 = df_cleaned_scaled.copy()\n",
        "del df_cleaned_scaled2['score']\n",
        "del df_cleaned_scaled2['score_scaled']\n",
        "df_cleaned_scaled3 = df_cleaned_scaled2.copy()"
      ],
      "metadata": {
        "id": "5SFfYLpJ8RL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate credit utilization ratio\n",
        "df_cleaned_scaled1['Credit_Utilization_Ratio'] = df_cleaned_scaled1['LoanAmount'] / df_cleaned_scaled1['Loan_Amount_Term']\n",
        "\n",
        "# Normalize credit utilization ratio to a 300-900 range (or any desired range)\n",
        "min_score = 300\n",
        "max_score = 900\n",
        "normalized_scores_utilization = ((1 - df_cleaned_scaled1['Credit_Utilization_Ratio']) * (max_score - min_score)) + min_score\n",
        "\n",
        "# Add normalized scores as a new column 'Credit_Score_Utilization' to the DataFrame\n",
        "df_cleaned_scaled1['Credit_Score_Utilization'] = normalized_scores_utilization\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(\"Updated DataFrame with Credit_Score_Utilization column:\")\n",
        "print(df_cleaned_scaled1[['LoanAmount', 'Loan_Amount_Term', 'Credit_Utilization_Ratio', 'Credit_Score_Utilization']])\n"
      ],
      "metadata": {
        "id": "fg6IZNdgZH6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned_scaled3.describe()"
      ],
      "metadata": {
        "id": "jQcvIx5T9YiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Altman Z-score\n",
        "Altman Z-score=1.2*(Total Assets/Working Capital)+1.4*(Total Assets/Retained Earnings)+3.3*(Total Assets/EBIT)+0.6*(Total Liabilities/Market Value of Equity)\n"
      ],
      "metadata": {
        "id": "tc64vDH6Yh5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate financial ratios based on dataset columns\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "\n",
        "\n",
        "df_cleaned_scaled3['Working_Capital'] = df_cleaned_scaled3['ApplicantIncome'] - df_cleaned_scaled3['LoanAmount']/df_cleaned_scaled3['Loan_Amount_Term']\n",
        "df_cleaned_scaled3['Retained_Earnings'] = df_cleaned_scaled3['ApplicantIncome'] / df_cleaned_scaled3['LoanAmount']\n",
        "df_cleaned_scaled3['EBIT'] = df_cleaned_scaled3['Loan_Amount_Term'] * df_cleaned_scaled3['Credit_History']\n",
        "df_cleaned_scaled3['Market_Value_of_Equity'] = df_cleaned_scaled3['Property_Area'] * df_cleaned_scaled3['Married']\n",
        "\n",
        "# Calculate Altman Z-score\n",
        "df_cleaned_scaled3['Altman_Z_Score'] = (1.2 * df_cleaned_scaled3['Working_Capital'] +\n",
        "                                       1.4 * df_cleaned_scaled3['Retained_Earnings'] +\n",
        "                                       3.3 * df_cleaned_scaled3['EBIT'] +\n",
        "                                       0.6 * df_cleaned_scaled3['Market_Value_of_Equity'])\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming df_cleaned_scaled3 is your DataFrame\n",
        "\n",
        "# Replace inf, -inf, and NaN values in 'Altman_Z_Score' column with -10\n",
        "df_cleaned_scaled3['Altman_Z_Score'].replace([np.inf, -np.inf, np.nan], -10, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "del df_cleaned_scaled3['Working_Capital']\n",
        "del df_cleaned_scaled3['Retained_Earnings']\n",
        "del df_cleaned_scaled3['EBIT']\n",
        "del df_cleaned_scaled3['Market_Value_of_Equity']\n",
        "df_cleaned_scaled3\n"
      ],
      "metadata": {
        "id": "OSPGE0S5_TvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate financial ratios based on dataset columns\n",
        "df_cleaned_scaled2['Working_Capital'] = df_cleaned_scaled2['ApplicantIncome'] - df_cleaned_scaled2['LoanAmount']/df_cleaned_scaled2['Loan_Amount_Term']\n",
        "df_cleaned_scaled2['Retained_Earnings'] = df_cleaned_scaled2['ApplicantIncome'] / df_cleaned_scaled2['LoanAmount']\n",
        "df_cleaned_scaled2['EBIT'] = df_cleaned_scaled2['Loan_Amount_Term'] * df_cleaned_scaled2['Credit_History']\n",
        "df_cleaned_scaled2['Market_Value_of_Equity'] = df_cleaned_scaled2['Property_Area'] * df_cleaned_scaled2['Married']\n",
        "\n",
        "# Calculate Altman Z-score\n",
        "df_cleaned_scaled2['Altman_Z_Score'] = (1.2 * (df_cleaned_scaled2['Working_Capital'] / df_cleaned_scaled2['ApplicantIncome']) +\n",
        "                                       1.4 * (df_cleaned_scaled2['Retained_Earnings'] / df_cleaned_scaled2['ApplicantIncome']) +\n",
        "                                       3.3 * (df_cleaned_scaled2['EBIT'] / df_cleaned_scaled2['ApplicantIncome']) +\n",
        "                                       0.6 * (df_cleaned_scaled2['Market_Value_of_Equity'] / df_cleaned_scaled2['LoanAmount']))\n",
        "\n",
        "# Display the updated DataFrame with Altman Z-score\n",
        "print(\"Updated DataFrame with Altman Z-score column:\")\n",
        "print(df_cleaned_scaled2[['Working_Capital', 'Retained_Earnings', 'EBIT', 'Market_Value_of_Equity', 'Altman_Z_Score']])\n",
        "\n"
      ],
      "metadata": {
        "id": "wFK3kqzeYxBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning the **df_cleaned_sclaed**"
      ],
      "metadata": {
        "id": "oy8LksfQfvh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "del df_cleaned_scaled2['Working_Capital']\n",
        "del df_cleaned_scaled2['Retained_Earnings']\n",
        "del df_cleaned_scaled2['EBIT']\n",
        "del df_cleaned_scaled2['Market_Value_of_Equity']\n",
        "\n"
      ],
      "metadata": {
        "id": "sX39TlLqfuiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned_scaled2=df_cleaned_scaled3.copy()"
      ],
      "metadata": {
        "id": "uighU-5fFfUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming df is your DataFrame with the 'Altman_Z_Score' column\n",
        "\n",
        "# Plotting the distribution using seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=df_cleaned_scaled2, x='Altman_Z_Score', bins=20, kde=True)\n",
        "plt.title('Distribution of Altman Z-Score')\n",
        "plt.xlabel('Altman Z-Score')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SI8kuc8z1SKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Assuming df_cleaned_scaled2 is your DataFrame with the provided features\n",
        "\n",
        "# Select features and target variable\n",
        "X = df_cleaned_scaled2[['Married', 'ApplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History', 'Property_Area']]\n",
        "y = df_cleaned_scaled2['Loan_Status']\n",
        "\n",
        "# Perform one-hot encoding on categorical features\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a logistic regression model\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = logreg.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Generate a classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate credit scores based on the logistic regression model\n",
        "df_cleaned_scaled2['Credit_Score'] = logreg.predict_proba(scaler.transform(X))[:, 1]\n",
        "\n",
        "# Print the DataFrame with Credit Score added\n",
        "print(df_cleaned_scaled2)\n"
      ],
      "metadata": {
        "id": "s6GIJi9q5bco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned_scaled2"
      ],
      "metadata": {
        "id": "RbalcdTYfipL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming df is your DataFrame with 'Altman_Z_Score' and 'Loan_Status' columns\n",
        "\n",
        "# Create a new DataFrame with the relevant columns\n",
        "df_cleaned_scaled2_plot = df_cleaned_scaled2[['Altman_Z_Score', 'Loan_Status']]\n",
        "\n",
        "# Plot the distribution using seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=df_cleaned_scaled2_plot, x='Altman_Z_Score', hue='Loan_Status', multiple='stack', bins=20)\n",
        "plt.title('Distribution of Loan Status along Altman Z-Score')\n",
        "plt.xlabel('Altman Z-Score')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Loan_Status')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-udAH_jUzhxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the linear regression model targetting the Altman_Z_Score"
      ],
      "metadata": {
        "id": "vYBR370al5gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned_scaled2 = df_cleaned_scaled2.dropna()\n",
        "df_cleaned_scaled3 = df_cleaned_scaled2.copy()"
      ],
      "metadata": {
        "id": "GdHrKh42_DYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Assuming df_cleaned_scaled contains your dataset\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df_cleaned_scaled2.drop(columns=['Altman_Z_Score'])  # Features\n",
        "y = df_cleaned_scaled2['Altman_Z_Score']  # Target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize linear regression model\n",
        "linear_reg = LinearRegression()\n",
        "\n",
        "# Fit the model on the training data\n",
        "linear_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict Altman Z-scores on the test set\n",
        "y_pred = linear_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared Score:\", r2)\n"
      ],
      "metadata": {
        "id": "i7l8opUzk-qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "qSCdDLrjl0GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the gradient boost regression model targetting the Altman_Z_Score"
      ],
      "metadata": {
        "id": "44Gcinf1m0DS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Gradient Boosting Regression model\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Predict Altman Z-scores on the test set using Gradient Boosting Regression\n",
        "y_pred_gbr = gbr.predict(X_test)\n",
        "\n",
        "# Evaluate the Gradient Boosting Regression model\n",
        "mse_gbr = mean_squared_error(y_test, y_pred_gbr)\n",
        "r2_gbr = r2_score(y_test, y_pred_gbr)\n",
        "\n",
        "print(\"Gradient Boosting Regression:\")\n",
        "print(\"Mean Squared Error:\", mse_gbr)\n",
        "print(\"R-squared Score:\", r2_gbr)"
      ],
      "metadata": {
        "id": "5s0SKIIKmFrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the support vector regression model targetting the Altman_Z_Score"
      ],
      "metadata": {
        "id": "gkhiItwem9ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svr = SVR()\n",
        "\n",
        "# Fit the model on the training data\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Predict Altman Z-scores on the test set using Support Vector Regression\n",
        "y_pred_svr = svr.predict(X_test)\n",
        "\n",
        "# Evaluate the Support Vector Regression model\n",
        "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
        "r2_svr = r2_score(y_test, y_pred_svr)\n",
        "\n",
        "print(\"\\nSupport Vector Regression:\")\n",
        "print(\"Mean Squared Error:\", mse_svr)\n",
        "print(\"R-squared Score:\", r2_svr)"
      ],
      "metadata": {
        "id": "xTEhabwMm8xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the decision tree regression model targetting the Altman_Z_Score"
      ],
      "metadata": {
        "id": "PHyNw9FKnLv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtr = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "dtr.fit(X_train, y_train)\n",
        "\n",
        "# Predict Altman Z-scores on the test set using Decision Tree Regression\n",
        "y_pred_dtr = dtr.predict(X_test)\n",
        "\n",
        "# Evaluate the Decision Tree Regression model\n",
        "mse_dtr = mean_squared_error(y_test, y_pred_dtr)\n",
        "r2_dtr = r2_score(y_test, y_pred_dtr)\n",
        "\n",
        "print(\"\\nDecision Tree Regression:\")\n",
        "print(\"Mean Squared Error:\", mse_dtr)\n",
        "print(\"R-squared Score:\", r2_dtr)"
      ],
      "metadata": {
        "id": "19AZSb9HnNJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating all previous models with a cross validation applied"
      ],
      "metadata": {
        "id": "SIqMIk-nAn-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "X = df_cleaned_scaled3.drop(columns=['Altman_Z_Score'])  # Features\n",
        "y = df_cleaned_scaled3['Altman_Z_Score']  # Target\n",
        "# Initialize Gradient Boosting Regression model\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Perform cross-validation for Gradient Boosting Regression\n",
        "gbr_cv_scores = cross_val_score(gbr, X, y, cv=5, scoring='neg_mean_squared_error')\n",
        "gbr_cv_mse_mean = -gbr_cv_scores.mean()\n",
        "\n",
        "# Initialize Support Vector Regression model\n",
        "svr = SVR()\n",
        "\n",
        "# Perform cross-validation for Support Vector Regression\n",
        "svr_cv_scores = cross_val_score(svr, X, y, cv=5, scoring='neg_mean_squared_error')\n",
        "svr_cv_mse_mean = -svr_cv_scores.mean()\n",
        "\n",
        "# Initialize Decision Tree Regression model\n",
        "dtr = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Perform cross-validation for Decision Tree Regression\n",
        "dtr_cv_scores = cross_val_score(dtr, X, y, cv=5, scoring='neg_mean_squared_error')\n",
        "dtr_cv_mse_mean = -dtr_cv_scores.mean()\n",
        "\n",
        "# Print mean MSE from cross-validation for each model\n",
        "print(\"Gradient Boosting Regression - Mean MSE:\", gbr_cv_mse_mean)\n",
        "print(\"Support Vector Regression - Mean MSE:\", svr_cv_mse_mean)\n",
        "print(\"Decision Tree Regression - Mean MSE:\", dtr_cv_mse_mean)"
      ],
      "metadata": {
        "id": "M4EQ8x275xIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Define the number of clusters (classes)\n",
        "num_clusters = 4\n",
        "\n",
        "# Initialize KMeans clustering model\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "\n",
        "# Fit KMeans model on the 'score_scaled' column\n",
        "kmeans.fit(df_cleaned_scaled[['score_scaled']])\n",
        "\n",
        "# Get cluster centers (representative values for each cluster)\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "# Sort the cluster centers to get the intervals for each class\n",
        "sorted_cluster_centers = sorted(cluster_centers.ravel())\n",
        "\n",
        "# Define class labels\n",
        "class_labels = ['poor', 'average', 'good', 'excellent']\n",
        "\n",
        "# Map each cluster center to its corresponding class label\n",
        "class_intervals = {}\n",
        "for i, label in enumerate(class_labels):\n",
        "    if i == 0:\n",
        "        class_intervals[label] = (-float('inf'), sorted_cluster_centers[i])\n",
        "    elif i == len(class_labels) - 1:\n",
        "        class_intervals[label] = (sorted_cluster_centers[i-1], float('inf'))\n",
        "    else:\n",
        "        class_intervals[label] = (sorted_cluster_centers[i-1], sorted_cluster_centers[i])\n",
        "\n",
        "# Assign class labels to each row based on the 'score_scaled' value\n",
        "df_cleaned_scaled['class'] = df_cleaned_scaled['score_scaled'].apply(lambda x: next(class_label for class_label, interval in class_intervals.items() if interval[0] <= x <= interval[1]))\n",
        "\n",
        "# Display the class intervals for each class label\n",
        "for class_label, interval in class_intervals.items():\n",
        "    print(f\"{class_label}: {interval}\")\n",
        "\n",
        "# Display the updated DataFrame with class labels\n",
        "print(\"Updated DataFrame with class labels:\")\n",
        "df_cleaned_scaled\n"
      ],
      "metadata": {
        "id": "sXbhz39XPKMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "KOFW5I1N-Hsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, completeness_score\n",
        "\n",
        "# Calculate silhouette score\n",
        "silhouette_avg = silhouette_score(df_cleaned_scaled[['score_scaled']], df_cleaned_scaled['class'])\n",
        "\n",
        "# Calculate completeness score\n",
        "completeness_avg = completeness_score(df_cleaned_scaled['class'], kmeans.labels_)\n",
        "\n",
        "# Display evaluation metrics\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "print(f\"Completeness Score: {completeness_avg}\")\n"
      ],
      "metadata": {
        "id": "IyLZpsC-Rpca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Assuming y_test contains continuous predictions (probabilities or scores)\n",
        "threshold = 0.5  # Adjust the threshold as needed\n",
        "\n",
        "# Convert to binary labels based on threshold\n",
        "y_pred_binary = (y_pred >= threshold).astype(int)\n",
        "y_test_binary = (y_test >= threshold).astype(int)\n",
        "\n",
        "# Now you can calculate metrics like accuracy and confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test_binary, y_pred_binary)\n",
        "accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "BPxDnTR7LFHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate clustering using various metrics\n",
        "silhouette_score = metrics.silhouette_score(X, kmeans.labels_)\n",
        "calinski_harabasz_score = metrics.calinski_harabasz_score(X, kmeans.labels_)\n",
        "davies_bouldin_score = metrics.davies_bouldin_score(X, kmeans.labels_)\n",
        "inertia = kmeans.inertia_\n",
        "\n",
        "print(\"Silhouette Score:\", silhouette_score)\n",
        "print(\"Calinski-Harabasz Index:\", calinski_harabasz_score)\n",
        "print(\"Davies-Bouldin Index:\", davies_bouldin_score)\n",
        "print(\"Inertia:\", inertia)"
      ],
      "metadata": {
        "id": "h_a7dvpNzARW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned_scaled"
      ],
      "metadata": {
        "id": "K5r_JRJNQbhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GMM"
      ],
      "metadata": {
        "id": "quZNg6bP3M1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Initialize Gaussian Mixture Model\n",
        "gmm = GaussianMixture(n_components=4, random_state=42)\n",
        "\n",
        "# Fit GMM model on the 'score_scaled' column\n",
        "gmm.fit(df_cleaned_scaled[['score_scaled']])\n",
        "\n",
        "# Evaluate the silhouette score for GMM\n",
        "gmm_silhouette_score = metrics.silhouette_score(df_cleaned_scaled[['score_scaled']], gmm.predict(df_cleaned_scaled[['score_scaled']]))\n",
        "\n",
        "print(\"Silhouette Score for Gaussian Mixture Model:\", gmm_silhouette_score)\n"
      ],
      "metadata": {
        "id": "Rphxk0Mm3Orp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Agg"
      ],
      "metadata": {
        "id": "Xs0Izdsd6s7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Initialize Agglomerative Clustering model\n",
        "agglomerative = AgglomerativeClustering(n_clusters=num_clusters)\n",
        "\n",
        "# Fit Agglomerative Clustering model on the 'score_scaled' column\n",
        "agglomerative.fit(df_cleaned_scaled[['score_scaled']])\n",
        "\n",
        "# Evaluate the silhouette score for Agglomerative Clustering\n",
        "agglomerative_silhouette_score = metrics.silhouette_score(df_cleaned_scaled[['score_scaled']], agglomerative.labels_)\n",
        "\n",
        "print(\"Silhouette Score for Agglomerative Clustering:\", agglomerative_silhouette_score)\n"
      ],
      "metadata": {
        "id": "ljimS3gE6unW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, silhouette_score\n",
        "\n",
        "# Compute additional metrics for KMeans\n",
        "kmeans_davies_bouldin = davies_bouldin_score(df_cleaned_scaled[['score_scaled']], kmeans.labels_)\n",
        "kmeans_calinski_harabasz = calinski_harabasz_score(df_cleaned_scaled[['score_scaled']], kmeans.labels_)\n",
        "kmeans_silhouette = silhouette_score(df_cleaned_scaled[['score_scaled']], kmeans.labels_)\n",
        "\n",
        "# Compute additional metrics for Agglomerative Clustering\n",
        "agglomerative_davies_bouldin = davies_bouldin_score(df_cleaned_scaled[['score_scaled']], agglomerative.labels_)\n",
        "agglomerative_calinski_harabasz = calinski_harabasz_score(df_cleaned_scaled[['score_scaled']], agglomerative.labels_)\n",
        "agglomerative_silhouette = silhouette_score(df_cleaned_scaled[['score_scaled']], agglomerative.labels_)\n",
        "\n",
        "# Compute additional metrics for GMM\n",
        "gmm_davies_bouldin = davies_bouldin_score(df_cleaned_scaled[['score_scaled']], gmm.predict(df_cleaned_scaled[['score_scaled']]))\n",
        "gmm_calinski_harabasz = calinski_harabasz_score(df_cleaned_scaled[['score_scaled']], gmm.predict(df_cleaned_scaled[['score_scaled']]))\n",
        "gmm_silhouette = silhouette_score(df_cleaned_scaled[['score_scaled']], gmm.predict(df_cleaned_scaled[['score_scaled']]))\n",
        "\n",
        "# Print the computed metrics\n",
        "print(\"KMeans:\")\n",
        "print(\"Davies-Bouldin Index:\", kmeans_davies_bouldin)\n",
        "print(\"Calinski-Harabasz Index:\", kmeans_calinski_harabasz)\n",
        "print(\"Silhouette Score:\", kmeans_silhouette)\n",
        "print()\n",
        "\n",
        "print(\"Agglomerative Clustering:\")\n",
        "print(\"Davies-Bouldin Index:\", agglomerative_davies_bouldin)\n",
        "print(\"Calinski-Harabasz Index:\", agglomerative_calinski_harabasz)\n",
        "print(\"Silhouette Score:\", agglomerative_silhouette)\n",
        "print()\n",
        "\n",
        "print(\"Gaussian Mixture Model (GMM):\")\n",
        "print(\"Davies-Bouldin Index:\", gmm_davies_bouldin)\n",
        "print(\"Calinski-Harabasz Index:\", gmm_calinski_harabasz)\n",
        "print(\"Silhouette Score:\", gmm_silhouette)\n"
      ],
      "metadata": {
        "id": "EAtgF99w6vxZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}